{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, balanced_accuracy_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install joblib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Perform Data Filtering and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provided_drug_name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lybrel</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethinyl estradiol / levonorgestrel</td>\n",
       "      <td>\"I had been on the pill for many years. When m...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nexplanon</td>\n",
       "      <td>\"Started Nexplanon 2 months ago because I have...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Etonogestrel</td>\n",
       "      <td>\"Nexplanon does its job. I can have worry free...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   provided_drug_name  \\\n",
       "0                              Lybrel   \n",
       "1                          Ortho Evra   \n",
       "3  Ethinyl estradiol / levonorgestrel   \n",
       "4                           Nexplanon   \n",
       "5                        Etonogestrel   \n",
       "\n",
       "                                              review  rating  \n",
       "0  \"I used to take another oral contraceptive, wh...       5  \n",
       "1  \"This is my first time using any form of birth...       8  \n",
       "3  \"I had been on the pill for many years. When m...       8  \n",
       "4  \"Started Nexplanon 2 months ago because I have...       3  \n",
       "5  \"Nexplanon does its job. I can have worry free...       9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.chdir(r\"C:\\Users\\Celeste\\Google Drive Streaming\\My Drive\\_CODEOP\\project\\data\")\n",
    "\n",
    "df = pd.read_csv('final_data.csv', encoding='utf-8')\n",
    "\n",
    "# Cleaning columns ans filtering \"Birth Control\"\n",
    "filtered_df = df[df['condition'] == 'Birth Control'].drop(\n",
    "    columns=['Unnamed: 0', 'condition', 'date', 'usefulCount', 'matched_generic_name', 'matched_type']\n",
    ")\n",
    "\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of DataFrame:\n",
      "--------------------------------------\n",
      "- Number of rows: 28788\n",
      "- Number of columns: 3\n",
      "- Column names: provided_drug_name, review, rating\n",
      "- Data types:\n",
      "provided_drug_name    object\n",
      "review                object\n",
      "rating                 int64\n",
      "- Non-null counts:\n",
      "provided_drug_name    28788\n",
      "review                28788\n",
      "rating                28788\n"
     ]
    }
   ],
   "source": [
    "summary = f\"\"\"\n",
    "Summary of DataFrame:\n",
    "--------------------------------------\n",
    "- Number of rows: {filtered_df.shape[0]}\n",
    "- Number of columns: {filtered_df.shape[1]}\n",
    "- Column names: {', '.join(filtered_df.columns)}\n",
    "- Data types:\n",
    "{filtered_df.dtypes.to_string()}\n",
    "- Non-null counts:\n",
    "{filtered_df.count().to_string()}\"\"\"\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.rename(columns= {\"provided_drug_name\":\"drugname\"}, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Function and Target Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label  pos and neg\n",
    "\n",
    "def label_rating(rating):\n",
    "    if rating <= 5:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drugname</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lybrel</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethinyl estradiol / levonorgestrel</td>\n",
       "      <td>\"I had been on the pill for many years. When m...</td>\n",
       "      <td>8</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nexplanon</td>\n",
       "      <td>\"Started Nexplanon 2 months ago because I have...</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Etonogestrel</td>\n",
       "      <td>\"Nexplanon does its job. I can have worry free...</td>\n",
       "      <td>9</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             drugname  \\\n",
       "0                              Lybrel   \n",
       "1                          Ortho Evra   \n",
       "3  Ethinyl estradiol / levonorgestrel   \n",
       "4                           Nexplanon   \n",
       "5                        Etonogestrel   \n",
       "\n",
       "                                              review  rating    target  \n",
       "0  \"I used to take another oral contraceptive, wh...       5  Negative  \n",
       "1  \"This is my first time using any form of birth...       8  Positive  \n",
       "3  \"I had been on the pill for many years. When m...       8  Positive  \n",
       "4  \"Started Nexplanon 2 months ago because I have...       3  Negative  \n",
       "5  \"Nexplanon does its job. I can have worry free...       9  Positive  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['target'] = filtered_df['rating'].apply(label_rating)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drugname    0\n",
       "review      0\n",
       "rating      0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "Positive    57.444074\n",
       "Negative    42.555926\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out target distribution\n",
    "\n",
    "target_distr = filtered_df[\"target\"].value_counts()\n",
    "target_distr*100/len(filtered_df)\n",
    "\n",
    "# 57% of the ratings are \"Positive\" and  43% are \"Negative\". A bit imbalanced dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This is my first time using any form of birth control. I&#039;m glad I went with the patch, I have been on it for 8 months. At first It decreased my libido but that subsided. The only downside is that it made my periods longer (5-6 days to be exact) I used to only have periods for 3-4 days max also made my cramps intense for the first two days of my period, I never had cramps before using birth control. Other than that in happy with the patch\"\n"
     ]
    }
   ],
   "source": [
    "# Seeing what reviews looks like\n",
    "text = filtered_df.review.iloc[1]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def RegEx\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(r\"[.;:!#\\'?,\\\"()\\[\\]&]|\\d+\")  # delete punctuation and numbers\n",
    "REPLACE_WITH_SPACE = re.compile(r\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\") \n",
    "\n",
    "# We want to remove everything that is not a proper character. RegEx for non-alphanumerical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess function for \"review\" column\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_review(reviews):\n",
    "\n",
    "    # Remove special characters\n",
    "    reviews = re.sub(REPLACE_NO_SPACE, \"\", reviews)  #aqui tambien podemos usar RegexpTokenizer()\n",
    "    reviews = re.sub(REPLACE_WITH_SPACE, \" \", reviews)\n",
    "\n",
    "    tokens = word_tokenize(reviews)\n",
    "\n",
    "    # Lowercase and lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmas = [lemmatizer.lemmatize(token.lower(), pos='v') for token in tokens if token.lower() not in stop_words]\n",
    "    lemmas = [lemmatizer.lemmatize(token.lower(), pos='n') for token in lemmas if token.lower() not in stop_words]\n",
    "\n",
    "\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check ____________________________________________________________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start position: 38\n",
      "End position: 81\n",
      "of birth control. I&#039;m glad I went with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['birth', 'control', 'im', 'glad', 'go']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = text.find(\"of birth control. I&#039;m glad I went with\")\n",
    "end = start + len(\"of birth control. I&#039;m glad I went with\")\n",
    "print(f\"Start position: {start}\")\n",
    "print(f\"End position: {end}\")\n",
    "text = filtered_df['review'][1][38:81]\n",
    "print(text)\n",
    "\n",
    "preprocess_review(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "____________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = independent variable = review,  y = target variable (dependent) = rating\n",
    "\n",
    "X = filtered_df['review']\n",
    "y= filtered_df['target']\n",
    "\n",
    "# why we are doing these? video: 1:20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame dimensions: (28788, 4)\n",
      "Train dimensions: ((23030,), (23030,))\n",
      "Test dimensions: ((5758,), (5758,))\n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame dimensions:\", filtered_df.shape)\n",
    "print(f\"Train dimensions: {X_train.shape, y_train.shape}\")  \n",
    "print(f\"Test dimensions: {X_test.shape, y_test.shape}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "Positive    13232\n",
      "Negative     9798\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "Positive    3305\n",
      "Negative    2453\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of classes in the training and testing sets\n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Both has the same proportion, 74% --> stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26256                                                     \"I switched to this pill after using Sprintec for years and had instant weight gain. I gained 20 pounds in the first three months. No noticeable changes in mood, sex drive, or acne. My skin stayed very clear. I also stopped having a period completely and although it was wonderful I could not get used to it. The pregnancy panics along with the weight gain forced me to stop taking these pills.\"\n",
       "22909                                         \"I&#039;ve had it for 2 weeks so far no side effects yay!!! I don&#039;t get why people keep it in for years if it has crazy side effects like weight gain, suicide etc? The moment I gain 5 lbs I&#039;m getting it out. I have a very very regular workout and diet schd. The only pills that worked for me we&#039;re Yasmin and mini pill but I kept forgetting to take it after my sons birth hence the implanon.\"\n",
       "17874                                                                                                                                                                                               \"I am 18 years old and after being on this pill it caused me to have extreme hair loss. I didn&#039;t realize this what was causing till 4 months in. It did a good job keeping me from getting pregnant but now I am depressed over my hair loss. Not worth it!\"\n",
       "3553     \"So...I had my 2nd baby and followed my same routine as before...had my 6 week check up, got on the mini pill b/c I am breastfeeding...then start having sex and going to the gym again...YAY!!! I gained 16 pounds in 3 weeks taking Eerin. I finished my 1st pack and never picked up my next prescription. I have an appointment to meet with my doctor to discuss taking other pills or the Depo shot. I have since lost all the rapid weight I gained.\"\n",
       "18426                     \"I had Nexplanon inserted on Nov. 20, 2016. It is Jan. 20, 2016 and I have bled every single day. There was a week of much heavier bleeding and maybe two weeks of moderate bleeding, the rest have been light to moderate without stopping. \\r\\nSome research suggests that Ibuprofen every 6 hours as prescribed or Doxycycline twice a day as prescribed will help to settle the endometrial lining however, neither has worked for me.\"\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing: Choosing the Right Weighting Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is my weighting scheme? in this case, Tfidf beacuse...\n",
    "\n",
    "vectoriser = TfidfVectorizer(analyzer=preprocess_review)\n",
    "X_train_tfidf = vectoriser.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23030, 11644)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape # --> representación de una Bag of Words (BoW)\n",
    "\n",
    "# had in mind: \n",
    "# DataFrame dimensions: (31251, 3)\n",
    "# Train dimensions: ((23030,), (23030,))\n",
    "# # Test dimensions: ((5758,), (5758,))\n",
    "\n",
    "# X_train_tfidf es una matriz donde cada fila representa un documento y cada columna representa una palabra (o término)\n",
    "# en el vocabulario que fue creado a partir de los documentos de entrenamiento.\n",
    "# Cada entrada en esta matriz corresponde a la importancia de la palabra en el documento, medida por el TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#represent test documents in the Vector Space we've learned with the train documents\n",
    "X_test_tfidf = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5758, 11644)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-Up: Running the Models Using the Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectoriser = TfidfVectorizer(analyzer=preprocess_review)\n",
    "# lr = LogisticRegression()\n",
    "\n",
    "# pipe_lr = Pipeline([\n",
    "#     ('vectorizer', vectoriser), \n",
    "#     ('classifier', lr)\n",
    "# ])\n",
    "\n",
    "# pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el pipeline de lr\n",
    "# joblib.dump(pipe_lr, 'pipe_lr.pkl')\n",
    "\n",
    "# Load  pipeline \n",
    "loaded_pipe_lr = joblib.load(r\"C:\\Users\\Celeste\\Google Drive Streaming\\My Drive\\_CODEOP\\project\\collaborative_project\\celes_copy\\pipe_lr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_lr = loaded_pipe_lr.predict(X_test) #for metrics as Precision, Recall and F1-score\n",
    "y_test_proba_lr = loaded_pipe_lr.predict_proba(X_test)[:,1] #useful in ROC-AUC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(analyzer=preprocess_review)\n",
    "# nb= MultinomialNB()\n",
    "\n",
    "# pipe_nb = Pipeline([\n",
    "#     ('vectoriser', vectoriser),\n",
    "#     ('classifier', nb)\n",
    "# ])\n",
    "\n",
    "# pipe_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving pipeline de nb\n",
    "#joblib.dump(pipe_nb, 'pipe_nb.pkl')\n",
    "\n",
    "# Load  pipeline \n",
    "loaded_pipe_nb = joblib.load(r\"C:\\Users\\Celeste\\Google Drive Streaming\\My Drive\\_CODEOP\\project\\collaborative_project\\celes_copy\\pipe_nb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_nb = loaded_pipe_nb.predict(X_test)\n",
    "y_test_proba_nb = loaded_pipe_nb.predict_proba(X_test)[:,1] #probabilidades de la clase positiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wrap Up Random Forest\n",
    "# vectoriser = TfidfVectorizer(analyzer=preprocess_review)\n",
    "# rf= RandomForestClassifier()\n",
    "\n",
    "# pipe_rf = Pipeline([('vectoriser', vectoriser),\n",
    "#                  ('classifier', rf)])\n",
    "\n",
    "# pipe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(pipe_rf, 'pipe_rf.pkl')\n",
    "\n",
    "loaded_pipe_rf = joblib.load(r\"C:\\Users\\Celeste\\Google Drive Streaming\\My Drive\\_CODEOP\\project\\collaborative_project\\celes_copy\\pipe_rf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_rf = loaded_pipe_rf.predict(X_test)\n",
    "y_test_proba_rf = loaded_pipe_rf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics for Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "y_test_pred_lr_encoded = label_encoder.transform(y_test_pred_lr)\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_accuracy_lr = balanced_accuracy_score(y_test_encoded, y_test_pred_lr_encoded)\n",
    "\n",
    "# F1 Score\n",
    "f1_score_lr = f1_score(y_test_encoded, y_test_pred_lr_encoded)\n",
    "\n",
    "# ROC AUC\n",
    "#roc_auc_lr = roc_auc_score(y_test_encoded, y_test_proba_lr[:, 1])  # Da error IndexError\n",
    "roc_auc_lr = roc_auc_score(y_test_encoded, y_test_proba_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (Logistic Regression): 0.8338654141120848\n",
      "F1 Score (Logistic Regression): 0.8604057099924869\n",
      "AUC (Logistic Regression): 0.9179436954841798\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "\n",
    "label_encoder_lr = LabelEncoder()\n",
    "y_test_encoded = label_encoder_lr.fit_transform(y_test)\n",
    "y_test_pred_lr_encoded = label_encoder_lr.transform(y_test_pred_lr)\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_accuracy_lr = balanced_accuracy_score(y_test_encoded, y_test_pred_lr_encoded)\n",
    "print(\"Balanced Accuracy (Logistic Regression):\", balanced_accuracy_lr)\n",
    "\n",
    "# F1 Score\n",
    "f1_score_lr = f1_score(y_test_encoded, y_test_pred_lr_encoded)\n",
    "print(\"F1 Score (Logistic Regression):\", f1_score_lr)\n",
    "\n",
    "# ROC AUC usando probabilidades\n",
    "roc_auc_lr = roc_auc_score(y_test_encoded, y_test_proba_lr)\n",
    "print(\"AUC (Logistic Regression):\", roc_auc_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (Naive Bayes): 0.7682262541837992\n",
      "F1 Score (Naive Bayes): 0.8406460296096905\n",
      "AUC (Naive Bayes): 0.9082310524085794\n"
     ]
    }
   ],
   "source": [
    "# Naive-Bayes\n",
    "\n",
    "label_encoder_nb = LabelEncoder()\n",
    "y_test_encoded = label_encoder_nb.fit_transform(y_test)\n",
    "y_test_pred_nb_encoded = label_encoder_nb.transform(y_test_pred_nb)\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_accuracy_nb = balanced_accuracy_score(y_test_encoded, y_test_pred_nb_encoded)\n",
    "print(\"Balanced Accuracy (Naive Bayes):\", balanced_accuracy_nb)\n",
    "\n",
    "# F1 Score\n",
    "f1_score_nb = f1_score(y_test_encoded, y_test_pred_nb_encoded)\n",
    "print(\"F1 Score (Naive Bayes):\", f1_score_nb)\n",
    "\n",
    "# ROC AUC using probabilidades\n",
    "roc_auc_nb = roc_auc_score(y_test_encoded, y_test_proba_nb)\n",
    "print(\"AUC (Naive Bayes):\", roc_auc_nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (Random Forest): 0.9137704117283909\n",
      "F1 Score (Random Forest): 0.9315392577258613\n",
      "AUC (Random Forest): 0.9811569149018183\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "# Codificación de etiquetas de prueba\n",
    "label_encoder_rf = LabelEncoder()\n",
    "y_test_encoded = label_encoder_rf.fit_transform(y_test)\n",
    "y_test_pred_rf_encoded = label_encoder_rf.transform(y_test_pred_rf)\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_accuracy_rf = balanced_accuracy_score(y_test_encoded, y_test_pred_rf_encoded)\n",
    "print(\"Balanced Accuracy (Random Forest):\", balanced_accuracy_rf)\n",
    "\n",
    "# F1 Score\n",
    "f1_score_rf = f1_score(y_test_encoded, y_test_pred_rf_encoded)\n",
    "print(\"F1 Score (Random Forest):\", f1_score_rf)\n",
    "\n",
    "# ROC AUC usando probabilidades\n",
    "roc_auc_rf = roc_auc_score(y_test_encoded, y_test_proba_rf)\n",
    "print(\"AUC (Random Forest):\", roc_auc_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Performance\n",
    "encoder=LabelEncoder()\n",
    "\n",
    "y_train_encoded=encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test) \n",
    "\n",
    "roc_auc_rf = roc_auc_score(y_test_encoded, y_test_proba_rf) \n",
    "\n",
    "print(\"AUC Random Forest:\", roc_auc_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Models' : ['Logistic Regresion', 'Naive-Bayes', 'Random Forest'], \n",
    "    'Precision': [0.85, 0.78, 0.90],\n",
    "    'Recall': [0.80, 0.75, 0.88],\n",
    "    'F1': [0.82, 0.76, 0.89],\n",
    "    'ROC AUC': [0.87, 0.73, 0.92]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_method = 'weighted'  # Should I use 'macro' or 'micro'?\n",
    "\n",
    "# # Choose micro when you want a global view of your model’s performance.\n",
    "# # Choose macro when you want to treat all classes equally, irrespective of their sizes.\n",
    "# # Choose weighted when dealing with imbalanced classes, as it takes class sizes into account.\n",
    "\n",
    "# # print(f'Accuracy: {accuracy_score(y_test, nb.predict(X_test_tfidf)):.2f}')  #REMOVE?\n",
    "# print(f'F1 Score: {f1_score(y_test, nb.predict(X_test_tfidf), average=average_method):.2f}')\n",
    "# print(f'Precision: {precision_score(y_test, nb.predict(X_test_tfidf), average=average_method):.2f}')\n",
    "# print(f'Recall: {recall_score(y_test, nb.predict(X_test_tfidf), average=average_method):.2f}')\n",
    "\n",
    "# # print(f'ROC AUC: {roc_auc_score(y_test, nb.predict_proba(X_test_tfidf)):.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "average='weighted':\n",
    " This parameter is used to compute the metrics for multi-class classification. It accounts for the proportion of each class in the dataset. can change it based on  specific needs (e.g., average='macro' or average='micro').\n",
    "Given that whe have a binary classification problem with a slight class imbalance (57% positive and 43% negative reviews), we select the next evaluation metrics for our model:\n",
    "\n",
    "**Precision**: is the proportion of true positive predictions among all positive predictions.\n",
    "\n",
    "In our context, precision will tell us how many of the reviews predicted as positive are actually positive. High precision means fewer false positives, which is important if you want to ensure that positive reviews are indeed seen as such.\n",
    "\n",
    "**Recall** (Sensitivity): proportion of true positive predictions among all actual positives. TP/P\n",
    "\n",
    "Will show how well our model identifies actual positive reviews. In a review context, high recall means that most positive reviews are correctly identified.\n",
    "\n",
    "**F1 Score**  (Relates precision and recall)\n",
    "\n",
    "The F1 score is particularly useful for balancing precision and recall, especially when have a slight imbalance. It gives a single score to evaluate your model’s performance, which is beneficial when want to optimize both false positives and false negatives.\n",
    "\n",
    "As an additional metric: **ROC-AUC**  Measures the model’s ability to distinguish between the positive and negative classes across all thresholds.\n",
    "Importance:  provides insights into how well your model performs across different thresholds, giving a broader view of its performance regardless of class imbalance.\n",
    "\n",
    "Why do not use accuracy --< While accuracy can provide a general sense of how well the model performs, it's not the best metric for imbalanced datasets. Since we have a slight imbalance, it might give a misleading impression of model performance, especially if the model predicts the majority class more often.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **y_test** -> true labels \n",
    " **nb.predict(X_test_tfidf)** -> The labels predicted by the model ()).\n",
    "\n",
    " **Accuracy** --> \n",
    " - It returns a value between 0 and 1 \n",
    " - Represents the proportion of correct predictions relative to the total predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
